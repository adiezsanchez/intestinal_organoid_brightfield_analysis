{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train and Setup</h1>\n",
    "\n",
    "The following notebook will run you through the preparation steps needed to train the APOC model (accelerated pixel object classifier) used to accurately find the organoids in-focus in your brightfield imaging experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import measure\n",
    "import napari\n",
    "from napari.utils import nbscreenshot\n",
    "from apoc import ObjectSegmenter, ObjectClassifier\n",
    "import pyclesperanto_prototype as cle  # version 0.24.1\n",
    "import numpy as np\n",
    "import napari_segment_blobs_and_things_with_membranes as nsbatwm  # version 0.3.6\n",
    "\n",
    "from utils import read_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Where are your files stored?</h2>    \n",
    "\n",
    "Below you should **edit the directory** containing your acquired images (z-stacks). Easiest thing will be to copy the folder containing your images inside the <code>./data</code> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing your files\n",
    "directory_path =\"./data/test_data\"\n",
    "\n",
    "# The following function will read all the images contained within the directory_path above\n",
    "# and store them grouped by well_id.\n",
    "images_per_well = read_images(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a check, it prints the grouped z-stacks\n",
    "for well_id, files in images_per_well.items():\n",
    "    print(f'Well {well_id}:')\n",
    "    for file_path in files:\n",
    "        print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will load an image and display it in a Napari viewer window, you can use said image to train the ObjectSegmenter needed to detect and segment the organoids in your analysis pipeline. You can change the image you are loading by editing the rgb_img variable according to the <code>well_id</code> and desired z-stack. i.e. <code>[\"B02\"][3]</code> chooses stack position z03 from well B02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one image and transform it into grayscale for APOC if needed\n",
    "rgb_img = tifffile.imread(images_per_well[\"A01\"][5], is_ome = False)\n",
    "if len(rgb_img.shape) < 3:\n",
    "    img = rgb_img\n",
    "elif rgb_img.shape[2] == 3:\n",
    "    img = rgb2gray(rgb_img)\n",
    "else:\n",
    "    print(\"Modify the loader to accommodate different file formats\", rgb_img.shape)\n",
    "viewer = napari.Viewer(ndisplay=2)\n",
    "viewer.add_image(img, name=\"Greyscale organoids\")\n",
    "nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train your APOC Organoid Object Segmenter</h2>\n",
    "\n",
    "Once the greyscale image has been opened on the Napari viewer you are ready to train the organoid object classifier. Then follow the next steps:\n",
    "\n",
    "1. Click on the **labels layer** on Napari to create a new label where you will paint some lines over your classes of interest (background and organoids).\n",
    "2. Select the paintbrush tool to paint over the Labels layer.\n",
    "3. Change the brush size to 5 (depending on your image, in general the thinner the lines the better). Note you are in label 1 (brown by default), we will use this to define the background class. Paint some lines over the background and around the organoids.\n",
    "4. Create a new class by clicking on the plus sign and paint over the organoids (choose in-focus and out-of-focus ones). Define the central areas and the edges.\n",
    "\n",
    "![Napari_screenshot](./images/organoid_labeled_layer.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're done scribbling over your image, on Napari upper menu go to <code>Tools > Segmentation-Labeling > Object Segmentation (APOC)</code>. Then follow the next steps:\n",
    "\n",
    "1. Select your greyscale organoids as the images used for training.\n",
    "2. Under the training tab select your Labels layer as the ground truth annotation.\n",
    "3. Choose 2 as the object class. 2 is the label you have used to annotate the organoids (in blue).\n",
    "4. Hit train!\n",
    "\n",
    "![Napari_screenshot](./images/APOC_organoid_labels_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking train you should see the following result. After training, Napari will generate an **ObjectSegmenter.cl** file inside your root folder. The next cell will load said file and apply the segmentation to your image. Check if it makes sense, you can correct the segmentation by going back to the Labels layer where you annotated the ground truth (brown and blue lines) and adding/erasing lines accordingly. Then click on the Object segmentation (APOC) right side panel click on Train again.\n",
    "\n",
    "![Napari_screenshot](./images/APOC_organoid_segmented.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply object segmenter from APOC\n",
    "segmenter = ObjectSegmenter(opencl_filename=\"./ObjectSegmenter.cl\")\n",
    "result = segmenter.predict(image=img)\n",
    "viewer.add_labels(result, name=\"Pre-processed organoid labels\")\n",
    "# Remove previous layers from the visualization\n",
    "viewer.layers[-2].visible = False\n",
    "viewer.layers[-3].visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing some holes in the organoid labels\n",
    "closed_labels = cle.closing_labels(result, None, radius=4.0)\n",
    "# closed_labels = np.array(closed_labels, dtype=np.int32) # Change dtype of closed labels to feed array into nsbatwm.split\n",
    "viewer.add_labels(closed_labels, name='Closed organoid labels (clesperanto)')\n",
    "# Remove previous layers from the visualization\n",
    "viewer.layers[-2].visible = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will allow you to visualize the segmented objects' area in pixels, this way you can manually select a threshold to remove unwanted small objects (debris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the area of each object to select a threshold to remove small objects\n",
    "# and keep just the organoids\n",
    "props = measure.regionprops(closed_labels) # Calculate the area of each labeled object\n",
    "\n",
    "# Extract area values\n",
    "area_values = [prop.area for prop in props]\n",
    "\n",
    "# Create and display the histogram\n",
    "plt.hist(area_values, bins=30, color='b', alpha=0.7)\n",
    "plt.xlabel('Area (pixels)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Object Areas')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below allows you to tune the threshold to remove small objects, by default anything below 1000px in area will be removed from the segmented objects list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude small labels, cutout in pixel area seems to be below 1000px\n",
    "exclude_small = cle.exclude_small_labels(closed_labels, None, 1000.0)\n",
    "exclude_small = np.array(exclude_small, dtype=np.int32) # Change dtype of closed labels to feed array into nsbatwm.split\n",
    "viewer.add_labels(exclude_small, name='Excluded small labels (clesperanto)')\n",
    "# Remove previous layers from the visualization\n",
    "viewer.layers[-2].visible = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should see something like this\n",
    "\n",
    "![Napari_screenshot](./images/remove_small_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will split organoids that are close to each other and wrongly segmented as one single object. You can play with the sigma value to see the effect on splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting organoids into a binary mask\n",
    "split_organoids = nsbatwm.split_touching_objects(exclude_small, sigma=10.0)\n",
    "viewer.add_labels(split_organoids, name='Splitted touching organoids (nsbatwm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Napari_screenshot](./images/binary_split_organoids.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will convert the binary mask of separated organoids into single objects again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected component (cc) labeling\n",
    "cc_split_organoids = nsbatwm.connected_component_labeling(split_organoids, False)\n",
    "viewer.add_labels(cc_split_organoids, name='Connected component organoids (scikit-image, nsbatwm)')\n",
    "# Remove previous layers from the visualization\n",
    "viewer.layers[-2].visible = False\n",
    "viewer.layers[-3].visible = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Napari_screenshot](./images/processed_organoid_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train your APOC Organoid Object Classifier</h2>\n",
    "\n",
    "The next steps will allow us to train the object classifier that is necessary to tell organoids in-focus from out-of-focus organoids. \n",
    "\n",
    "1. Create a new Labels layer.\n",
    "2. Double click on it and rename it as Focus labels. Hide the Connected components label by clicking-on the eye icon.\n",
    "3. Draw brown lines (label 1) over out of focus organoids. Remember the thinner the lines the better (brush size <=5)\n",
    "4. Increase label id to 2 by cliking on the + sign and draw over in-focus organoids.\n",
    "\n",
    "You should end up with something looking like this:\n",
    "\n",
    "![Napari_screenshot](./images/organoid_focus_labeled_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with the amount of labelling (you do not need much) on Napari upper menu go to <code>Tools > Segmentation post-processing > Object Classification (APOC)</code>. Then follow the next steps:\n",
    "\n",
    "1. On the Labels tab choose Connected component organoids as the layer we will apply the object classifier to.\n",
    "2. Under the Training tab choose Focus labels as the annotation. Then click on all the options displayed in the screenshot below.\n",
    "3. Hit Train.\n",
    "\n",
    "![Napari_screenshot](./images/APOC_organoid_focus_labels_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking train you should see the following result. After training, Napari will generate an **ObjectClassifier.cl** file inside your root folder. The next cell will load said file and apply the classification to your organoid objects. Check if it makes sense, you can correct the segmentation by going back to the Focus Labels layer where you annotated the ground truth (brown and blue lines) and adding/erasing lines accordingly.\n",
    "\n",
    "![Napari_screenshot](./images/APOC_organoids_focus_classified.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply object classifier from APOC\n",
    "classifier = ObjectClassifier(opencl_filename=\"./ObjectClassifier.cl\")\n",
    "result = classifier.predict(labels=cc_split_organoids, image=img)\n",
    "viewer.add_labels(result, name=\"Focus and out of focus organoids\")\n",
    "# Remove previous layers from the visualization\n",
    "viewer.layers[-2].visible = False\n",
    "viewer.layers[-3].visible = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ObjectClassifier and Segmenter are ready!</h2>\n",
    "\n",
    "At this point you should have generated the classifier and segmenter files needed to run the second Jupyter notebook [2_find_focus_and_store](./2_find_focus_and_store.ipynb) that will allow you to analyze your stack of images and choose the one with the highest number of organoids in focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resulting .cle image into a np.array to count objects within each class\n",
    "\n",
    "image_array = np.array(result, dtype=np.int8)\n",
    "# Create masks for each class\n",
    "out_of_focus_mask = image_array == 1\n",
    "in_focus_mask = image_array == 2\n",
    "\n",
    "# Label connected components in each mask\n",
    "out_of_focus_labels = measure.label(out_of_focus_mask, connectivity=2)\n",
    "in_focus_labels = measure.label(in_focus_mask, connectivity=2)\n",
    "\n",
    "# Calculate the number of objects in each class\n",
    "num_out_of_focus_objects = np.max(out_of_focus_labels)\n",
    "num_in_focus_objects = np.max(in_focus_labels)\n",
    "\n",
    "print(f\"Number of Out-of-Focus Objects: {num_out_of_focus_objects}\")\n",
    "print(f\"Number of In-Focus Objects: {num_in_focus_objects}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbn39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
